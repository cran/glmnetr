---
title: "Calibration of Machine Learning Models in glmnetr"
author: "Walter K. Kremers, Mayo Clinic, Rochester MN"
date: "28 August 2024"
output: pdf_document 
vignette: >
  %\VignetteIndexEntry{An Overview of glmnetr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
#library( glmnetr )
#library(survival)
source("~/Documents/Rpackages/glmnetr_supl/_load_glmnetr_vig_240828.R" , echo=TRUE)
set.seed(116291949) 
simdata=glmnetr.simdata(nrows=1000, ncols=100, beta=NULL)
xs = simdata$xs                           # matrix of predictors 
yt = simdata$yt                           # vector of Gaussian (normal) outcomes
yg = simdata$y_                           # vector of Gaussian (normal) outcomes
yb = simdata$yb                           # vector of yes/no (binomial) outcomes
event = simdata$event
load( file="~/Documents/Rpackages/glmnetr_supl/using_glmnetr_outputs_240827.RLIST" ) ## 240828 
```
# The Package 
The "An Overview of glmnetr" vignette shows how to run the main package function nested.glmnetr() and how to summarize model performances.  If one identifies a well performing model according to the metrics in this summary, e.g. concordance, correlation, deviance ratio, linear calibration, one may want to do further evaluation in terms of calibration.  The strongest calibration and validation will involve calibration with new, independent datasets.  Frequently one will not have immediate access to such new data sets, or one may want first to do an internal validation before subjecting a model to an external validation.  Here we consider an internal validation approach using cross validation or bootstrap re-sampling, similar to how we numerically assessed model performance.

# An example analysis 
To explore calibration we first consider the nested.glmnetr() call from the "An Overview of glmnetr" vignette which fit machine learning models to survival data with family="cox", i.e. 
```{r Fit a relaxed lasso model informed by cross validation for cox, include=TRUE, eval=FALSE}
set.seed(465783345) 
nested.cox.fit = nested.glmnetr(xs, NULL, yt, event, family="cox", 
                                dolasso=1, dostep=1, steps_n=40, folds_n=10, track=1)  
```
```{r Fit a relaxed lasso model informed by cross validation for cox post, include=FALSE, eval=TRUE}
nested.cox.fit$fits[c(2:5,7,8)] = 0 
```
# Linear calibration 
Using either print() or summary() on the output object nested.cox.fit one gets, amongst other information, summaries for the linear calibration slopes and intercepts as in 
```{r general model performance cox, include=TRUE, eval=TRUE}
summary( nested.cox.fit ) 
```
Here we see that for many of the models the linear calibration slope term is near 1, the ideal for perfect calibration.  For the Cox model any intercept term can be absorbed into the baseline survival function and there is no pertinent intercept term for calibration. 

# A first visual
An initial calibration consideration was made in the overview vignette by regressing observed outcomes on the predicteds from the final model based upon the relaxed lasso.  This regression was made using splines, in particular the pspline() function from within a coxph() call, as in
```{r Naive alibration plot}
# Get predicteds from CV relaxed lasso model embedded in nested CV outputs & Plot
xb.hat = predict( object=nested.cox.fit , xs_new=xs, lam=NULL, gam=NULL, comment=FALSE) 
# Fit a spline to xb.hat uisng coxph, and plot 
#library( survival )                  ## load survival package for Cox model fits 
fit1 = coxph(Surv(yt, event) ~ pspline(xb.hat))
```
```{r print before plot, include=TRUE, eval=TRUE}
summary(fit1)
```
followed by plotting with 
```{r Niave calibration termplot}
termplot(fit1,term=1,se=TRUE, rug=TRUE)
abline(a=0,b=1,lty=3)
```
The spline fits may help to understand potential nonlinearities in the model.  Here we see, a clibration line which is not far from linear.  Still, as noted in the "An Overview of glmnetr" vignette, because the same data are used for model evaluation as well as model derivation, it is hard to put much confidence in such a calibration plot because of potential bias which may suggest a better fit than can be expected for new data.

# Calibration using spline fits and resampling 
For each of the models fit, nested.glmnetr() saves the X* Beta's from the final model.  The nested.glmentr() function also calculates the X* Betas's for the hold out data for each partitioning, i.e. each hold out fold of the outer loop of nested cross validation or the out-of-bag items not selected by the sample whit replacement of the bootstrap sample.  In this manner there are multiple subsets, e.g. k from the k-fold nested CV, or calculation of X*Betas based upon independent observations, and each of these subsets can contribute to calibrate the final model.  While each of these calibrations will individually have limited information, when combined following the principles of cross validation for boostrap sampling, they will collectively provide a more meaningful evaluation.  This is done by the calplot() function as in 
```{r calplot 1 }
calplot(nested.cox.fit, wbeta=5)
```
Here we see a smooth, nearly linear predicted log hazard ratio as a function of the model X* Beta from the relaxed lasso model.  The bounding lines in red depict the average +/- 2 standard errors (SE) to assist in assessing meaningfulness in any deviation from the ideal identity line, and non linearities.  In these curves the central region with solid lines denotes the region within the range of all the calibration spline fit, i.e. spline fits from all the different leave-out folds of the CV overlap without extrapolation.  The dashed lines depict areas out of range for at least one of the leave out folds.  Because spline fits can be rather uncertain when extrapolating beyond the data range, one should be more cautious in making strong conclusions in the dashed regions of these plots.  

In this figure we see two rugs, one below and one above the plotted region.  The rug below depicts the model X* Beta's which are not associated with an event and the rug above depicts X* Beta's which are associated with events.  When there are lots of data points it can be hard to read these rugs.  One can use the vref option in calplot to draw two vertical lines where the first separates the smaller vref% of the X* Beta's form the rest, and a second which separates the larger vref% of the data.  To depict the hazard ratios (HR) instead of the X*Beta for the Cox model one can use the option plothr, where one assigns a numerical value for the product between tick marks, e.g. exp(1) or 10.  Combining these two options we have the example 
```{r calplot 2 }
calplot(nested.cox.fit, wbeta=5, vref=1, plothr=100)
```
The user can also use different colors for the lines with the options col.term, col.se.  One can also specify xlim and ylim in case a few data points cause an excessive amount of white space or odd aspect ratio in the plots.

To view the calibration plots form the individual leave out cross validation folds, one may specify foldplot= 1.  In that this generates many figures, we omit in this vignette actually producing plots using this option specification, and instead assign plotfold=1 which overlays the individual calibration curves, albeit without the +/- 2 SE limits for the individual CV folds.  The overall calibration (average of the individual CV fold calibrations) and overall +/- 2 SE limits though are maintained. 
\newpage
```{r Fit a relaxed lasso model informed by cross validation for cox post 2, include=FALSE, eval=TRUE}
nested.cox.fit$xbetas.cv = nested.cox.fit$xbetas.cv[,c(1:15,26,27)] 
```
```{r calplot 3 }
calplot(nested.cox.fit, 5, plotfold=1)
```
As we see from the above calls the first term in the calplot() function call is an output object form a nested.glmnetr() call. The second term, wbeta, specifies "which beta" or model is to be used for deriving the model X*Beta's.  Here, as we see in the figure x-axis label, the 5 determines the relaxed lasso model.  Instead of making a hard to remember key the user can leave this term unspecified and a key will be directed to the R console.  The actual numbers for the different models will depend on which models are fit and so this key is dynamic.
```{r calplot key }
calplot(nested.cox.fit)
```
From this key we read of the numers corresponding to the respective models. The variance in X*beta for the "null" model is 0 as the intercept for the Cox model is arbitrarily assinged the value of 0 for each resample model fit. From this key we see we can produce a calibration plot for the ridge regression model by setting wbeta = 8 (wbeta for "which beta"), as in
```{r calplot 4 }
calplot(nested.cox.fit, 8)
```
Here we see the model is not ideally calibrated as the calibration curve largely does not include the identity line, and it requires a correction to achieve an un (less) biased estimation of the hazard ratio. Inspecting the calibration curve for a step wise regression model
\newpage
```{r calplot 5 }
calplot(nested.cox.fit, 16)
```
we see that the response is roughly linear but numerically at least there seems to be some correction for over fitting.

To obtain the numerical values used to construct these calibration plots one may specify plot=0 (or plot=2 to plot and obtain the numerical data) in list format as in
```{r calplot 6 }
tmp = calplot(nested.cox.fit, 5, plot=0)
str(tmp)
```
These data may be further processed by the user.  

\newpage
# A Binomial Model 
For nested.glmnetr() analyses with family = "binomial" with the call
```{r  Nested cross-validation evaluation for multiple machine learning models - binomial, include=TRUE, eval=FALSE}
yb = simdata$yb 
nested.bin.fit = nested.glmnetr(xs,NULL,yb,NULL,family="binomial",
      dolasso=1, doxgb=list(nrounds=250), dorf=1, doorf=1, doann=1, 
      folds_n=10, seed=219301029, track=1) 
```

an example calibration plot is 
```{r  Calibration binomial eval, warning=FALSE, error=FALSE, eval=TRUE}
calplot(nested.bin.fit, 5, plotfold=0) 
```
Since these data were generated with probabilities exp(X*beta)/(1+exp(X*beta)) we want expect that the medal would calibrate linearly.  Next we look at the "fully" relaxed model where an unpenalized model is fit based upon the non-zero terms in the fully penalized lasso model.  

```{r  Calibration binomial eval boot, warning=FALSE, error=FALSE, eval=TRUE}
calplot(nested.bin.fit, 7, plotfold=0) 
```
This may calibrate slightly better but is not as linear as we might expect.

# A Binomial Model Calibrated using Bootstrap 
Considering this we next fit models using bootstrap, that is fit models based upon random samples from the original sample (with replacement) of size the same as the original sample. Then we fit calibration curves for the out-of-bag sample units for each bootstrap sample, that is the elements of the original sample that are not selected by the bootstrap sample. This is done by specifying the number of bootstrap samples for calculation with the bootstrap option in the calplot() call.  First we perform the bootstrap model generation and Out Of Bag (OOB) performance calculations using the nested.glmnetr() function with the _boostrap_ option,
```{r  Nested cross-validation evaluation for multiple machine learning models - binomial - boot, include=TRUE, eval=FALSE}
yb = simdata$yb 
nested.bin.boot.fit = nested.glmnetr(xs,NULL,yb,NULL,family="binomial",
      dolasso=1, doorf=1, 
      folds_n=10, seed=219301029, track=1, bootstrap=20) 
```

with resulting plot 
```{r  Calibration binomial eval -boot, warning=FALSE, error=FALSE, eval=TRUE}
calplot(nested.bin.boot.fit, 5, plotfold=0) 
```

Knowing by the data construction that the true relation between predictors and outcome is linear, it appears that the bootstrap might be better at approximating this linearity.  

# A Normal (Gaussian errors) Model
First calculating the numerical summaries of prediction performance 
```{r  Nested cross-validation evaluation for multiple machine learning models-gaussian, include=TRUE, eval=FALSE}
nested.gau.fit = nested.glmnetr(xs,NULL,y_,NULL,family="gaussian",
      dolasso=1, doxgb=list(nrounds=250), dorf=1, doorf=1, doann=list(bestof=10), 
      folds_n=10, seed=219301029, track=1) 
```
and then plotting
```{r  Nested cross-validation calibration plot - gaussian, include=TRUE, eval=TRUE}
calplot(nested.gau.fit, wbeta=7)
```
we see a small but probably not significant deviation from the ideal calibration line, the identity function. 

# Perspective
The summary and calibration plot functions used here do not address all needed for model validation and calibration but do allow a meaningful and un (or minimally) biased summary of model fits.
