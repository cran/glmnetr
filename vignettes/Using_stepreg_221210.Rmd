---
title: "Using stepreg"
author: "Walter K. Kremers, Mayo Clinic, Rochester MN"
date: "10 December 2022"
output: pdf_document 
vignette: >
  %\VignetteIndexEntry{Using stepreg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(survival)
library(glmnet)
library(Matrix)
source("~/Documents/LASSO/glmnetr/R/aicreg_221210.R"                , echo=TRUE)
source("~/Documents/LASSO/glmnetr/R/cv.glmnetr_221210.R"            , echo=TRUE)
source("~/Documents/LASSO/glmnetr/R/cv.stepreg_221210.R"            , echo=TRUE)
source("~/Documents/LASSO/glmnetr/R/nested.glmnetr_221210.R"        , echo=TRUE)
source("~/Documents/LASSO/glmnetr/R/plot.cv.glmnetr_221210.R"       , echo=TRUE)
source("~/Documents/LASSO/glmnetr/R/stepreg_221210.R"               , echo=TRUE)
source("~/Documents/LASSO/glmnetr/R/summary.cv.glmnetr_221210.R"    , echo=TRUE)
source("~/Documents/LASSO/glmnetr/R/summary.nested.glmnetr_221210.R", echo=TRUE)
```
# The Package
|     The stepreg() and cv.stepreg() funcitons in the _glmnetr_ package were written for convenience and stability as opposed to speed or broad applicability.  When fitting lasso models we wanted to compare these to standard stepwise regression models.  Keeping a more modern approach we tune by either number of terms included in the model (James, Witten, Hastie and Tibshirani, An Introduction to Statistical Learning with applications in R, 2nd ed., Springer, New York, 2021) or by the p critical value for  model inclusion, as this too is a common tuning parameter when fitting stepwise models.  
|     When fitting lasso models we often use one-hot coding for predictor factors when setting up the design matrix.  This allows lasso to identify and add to the model a term for any one group that might be particularly different from the others.  By the penalty lasso stabilizes the model coefficients and keeps them form going to infinite, while ridge will generally uniquely identify coefficients despite any strict collinearities.  
|     Before writing this program we tried different available packages to fit stepwise models for the Cox repregreison framework but all we tried had difficulties with numerical stability for the large and wide clinical datasets we were working with, and  which involved one-hot coding.  There may well be a package that would be stable for the data we were analyzing but we decided to write this small function to be able to tune for stability.  
|     This program is slow but our goal was not for routine usage but to use the stepwise procedure on occasion as a reference for the lasso models.  For many clinical datasets the lasso clearly outperformed the stepwise procedure, and ran much faster.  For many simulated data sets with simplified covariance structures, i.e. independence of the underlying predictors, the lasso did not appear to do much better than the stepwise procedure tuned by number of model terms or p.   

# Data requirements 
|     The data requirements for stepreg() and cv.stepreg() are similar to those of cv.glmnetr() and refer to the _Using glmneter_ vignette for a description.

# An example dataset 
|     To demonstrate usage of _cv.stepreg_ we first generate a data set for analysis, run an analysis and evaluate.  Following the _Using glmnetr_ vignette, the code 
```{r Simuate survival data}
# Simulate data for use in an example survival model fit
# first, optionally, assign a seed for random number generation to get applicable results  
set.seed(116291949) 
simdata=glmnetr.simdata(nrows=1000, ncols=100, beta=NULL)
```
generates simulated data for analysis.  We extract data in the format required for input to the _cv.stepreg_ (and _glmnetr_) programs.
```{r Extract data from the simulation output object}
# Extract simulated survival data 
xs = simdata$xs        # matrix of predictors 
y_ = simdata$yt        # vector of survival times 
event = simdata$event  # indicator of event vs. censoring
```
Inspecting the predictor matrix we see 
```{r Inspect xs, the design matrix}
# Check the sample size and number of predictors
print(dim(xs)) 
# Check the rank of the design matrix, i.e. the degrees of freedom in the predictors 
rankMatrix(xs)[[1]]
# Inspect the first few rows and some select columns 
print(xs[1:10,c(1:12,18:20)])
```
# Cross validation (CV) informed stepwise model fit
|     To fit a relaxed lasso model and get reasonable hyperparameters for lambda and gamma, and summarize the cross-validated "tuned" model fit, we can use the function cv.glmnet() function. 
```{r  Fit a stepwise regression model informed by cross validation }
# Fit a stepwise regression model informed by cross validation 
cv.stepwise.fit=suppressWarnings(cv.stepreg(xs,NULL,y_,event,family="cox",folds_n=5,steps_n=30)) 
```
Note, in the derivation of the stepwise regression models, individual coefficients may be unstable even when the model may be stable which elicits warning messages.  Thus we "wrapped" the call to cv.stepreg() within the suppressWarnings() function to suppress excessive warning messages in this vignette.   The first term in the call to cv.stepreg(), xs, is the design matrix for predictors.  The second input term, here NULL, is for the start time in case (start, stop) time data setup is used in a Cox survival model. The third term is the outcome variable for the linear regression or logistic  regression model and the time of event or censoring in case of the Cox model, and finally the forth term is the event indicator variable for the Cox model taking the value 1 in case of an event or 0 in case of censoring at time y_.  The forth term would be NULL for either linear or logistic regression.  Currently the options for family are "guassian" for linear regression , "binomial" for logistic regression (both using the _stats_ glm() function) and "cox" for the Cox proportional hazards regression model using the coxph() function of the R _survival_ package.  |
    To summarizing the model fit and inspect the coefficient estimates we use the summary() function.  
```{r  summarize cross validation informed model fits }
# summarize model fit ...  
summary(cv.stepwise.fit)
```
# Nested cross validation 
|     Because the values for lambda and gamma informed by CV are specifically chosen to give a best fit, model fit statistics for the CV derived model will be biased.  To address this one can perform a CV on the CV derived estimates, that is a nested cross validation as argued for in SRDM ( Simon R, Radmacher MD, Dobbin K, McShane LM. Pitfalls in the Use of DNA Microarray Data for Diagnostic and Prognostic Classification. J Natl Cancer Inst (2003) 95  (1):  14-18. https://academic.oup.com/jnci/article/95/1/14/2520188 ).  This is done here by the nested.glmnetr() function.   

```{r  Nested cross-validation model evaluation}
# A nested cross validation to evaluate cross validation informed lasso and stepwise fit 
y_ = simdata$y_
nested.cox.fit = suppressWarnings(nested.glmnetr(xs,NULL,y_,NULL,family="gaussian",
                                  dolasso=1,dostep=1,doaic=1,folds_n=3,steps_n=30)) 
#nested.cox.fit= suppressWarnings(nested.glmnetr(xs,NULL,y_,event,family="cox",
#                                 dolasso=1,dostep=1,doaic=1,folds_n=3,steps_n=30)) 
summary(nested.cox.fit)
#names(nested.cox.fit)
```
For this example we use only 3 folds, instead of 5 or 10 like we would do in practice, to get reasonble run times as this is just for the purpose of demostration.  
|     Before providing analysis results the output first reports sample size and since this is for a Cox regression, the number of events, followed by the number of predictors and the df (degrees of freedom) of the design matrix, as well as some information on "Tuning parameters" to compare the lasso model with stepwise procedures as described in JWHT (James, Witten, Hastie and Tibshirani, An Introduction to Statistical Learning with applications in R, Springer, New York, 2021).  In general we have found in practice that the lasso performs better.    
|     Next are the nested cross validation results.  First are the per record (or per event in case of the Cox model) log-likelihoods which reflect the amount of information in each observation.  Since we are not using large sample theory to base inferences we feel the per record are more intuitive, and they allow comparisons between datasets with unequal sample sizes.  Next are the average number of model terms which reflect the complexity of the different models, even if in a naive sense, followed by the agreement statistics, here concordance,  These nested cross validated concordances should be essentially unbiased for the given design, unlike the naive concordances where the same data are used to derive the model and calculate the concordances (see SRDM). 
|     In addition to evaluating the CV informed model fits using another layer of CV, the nested.glmnetr() function does the CV fits based upon the whole data set.  Here we see, not unexpectedly, that the concordances estimated from the nested CV are slightly smaller than the concordances naively calculated using the original dataset.  Depending on the data the nested CV and naive agreement measures, here R-square, can be very similar or disparate.  
|     A summary for the CV fit can be produced by extracting the object$cv.stepreg.fit, where object is the output object obtained when running nested.glmnetr(), and then using the summary() function.  
```{r  Nested cross-validation model fit }
# Summary of the CV informed stepwise model fit
summary(nested.cox.fit$cv.stepreg.fit)
#names(nested.cox.fit)
```
