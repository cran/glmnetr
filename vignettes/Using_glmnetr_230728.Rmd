---
title: "Using glmnetr"
author: "Walter K. Kremers, Mayo Clinic, Rochester MN"
date: "28 May 2023"
output: pdf_document 
vignette: >
  %\VignetteIndexEntry{Using glmnetr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(survival)
library(Matrix)
library(rpart)
library(torch)
source("~/Documents/LASSO/glmnetr/R/_load_glmnetr_dev_230724.R" , echo=TRUE)
```
# The Package
|     The _glmnetr_ packages fits ralxed lasso, artificial neural network (NN), gradient boosting machine (GBM), resursive partitioning and stepwise regression models, all with hyperparameters informed by cross validation.  It fits all these model as extensions of linear, logistic and Cox regression models.  The package can fit all these models in a single call, and performs nested cross validation allowing the user to evaluate and compare the performances of these different models.  The package fits these models using other r packages including _glmnet_, _survival_, _xgboost_, _rpart_ and _torch_.  For the relaxed lasso models _glmnetr_ uses R _stat_ and _survival_ to obtain stable model fits, and obtain these often more quickly.  This too might be achieved using the 'path=TRUE' option in _glmnet_.      
|     While the package fits nested cross validation for the lasso and other models, it does not fit the general elastic net model.  If you are fitting not a relaxed lasso model but an elastic-net model, then the R-packages _nestedcv_ ( https://cran.r-project.org/package=nestedcv ), 'glmnetSE' ( https://cran.r-project.org/package=glmnetSE ) or others may provide greater functionality when performing a nested CV.
|     As with the _glmnet_ package, this package passes most relevant information to the output object which can be evaluated using plot, summary() and predict() functions.  The _glmnetr_ package has some features and functionality that we find useful, but omits some of the functionality of _glmnet_ as well.  Use of the _glmnetr_ package has many similarites to the _glmnet_ package and it is recommended that the user of _glmnetr_ first become familiar with the _glmnet_ package ( https://cran.r-project.org/package=glmnet ), with the "An Introduction to glmnet" and "The Relaxed Lasso" being especially helpful in this regard.      
# Data requirements 
|     The basic data elements for input to the _glmnetr_ analysis programs are similar to those of _glmnet_ and include 1) a matrix of predictors and 2) an outcome variable or variables in vector form.  For the estimation of the "fully" relaxed models (where gamma=0) the package is set up to fit the "gaussian" and "binomial" models using the _stats_ glm() function and Cox survival models using the the coxph() function of the _survival_ pacakge.  When fitting these extensions to the Cox model the outcome model variable is interpreted as the "time" variable in the Cox model, and one must also specify 3) a variable for event, again in vector form, and optionally 4) a variable for start time, also in vector form.  Row i of the predictor matrix and element i of the outcome vector(s) are to include the data for the same sampling unit.
# An example dataset 
|     To demonstrate usage of _glmnetr_ we first generate a data set for analysis, run an analysis and evaluate using the plot(), summary() and predict() functions.    
|     The code 
```{r Simuate survival data}
# Simulate data for use in an example relaxed lasso fit of survival data
# first, optionally, assign a seed for random number generation to get replicable results  
set.seed(829662260)
simdata=glmnetr.simdata(nrows=1000, ncols=100, beta=NULL, intr=c(1,0,1,1))
```
generates simulated data for analysis.  We extract data in the format required for input to the _glmnetr_ programs as in
```{r Extract data from the simulation output object}
# Extract simulated survival data 
xs = simdata$xs        # matrix of predictors 
y_ = simdata$yt        # vector of survival times 
event = simdata$event  # indicator of event vs. censoring
```
Inspecting the predictor matrix we see 
```{r Inspect xs, the design matrix}
# Check the sample size and number of predictors
print(dim(xs)) 
# Check the rank of the design matrix, i.e. the degrees of freedom in the predictors 
rankMatrix(xs)[[1]]
# Inspect the first few rows and some select columns 
print(xs[1:10,c(1:12,18:20)])
```
# Cross validation (CV) informed relaxed lasso model fit
|     To fit a cross-validated "tuned" relaxed lasso model can use the cv.glmnetr() function. 
```{r  Fit a relaxed lasso model informed by cross validation }
# Fit a relaxed lasso model informed by cross validation 
cv.cox.fit = suppressWarnings( cv.glmnetr(xs, NULL, y_, event, family="cox", track=0) ) 
```
Note, in the derivation of the relaxed lasso model fits, individual coefficients may be unstable even when the model may be stable which elicits warning messages.  Thus we "wrapped" the call to cv.glmnetr() within the suppressWarnings() function to avoid excessive warning messages in this vignette.  The first input term in the call to cv.glmnetr(), xs, is the design matrix for predictors.  The second input term, here NULL, is for the start time in case (start, stop) time data setup is used in a Cox survival model. The third term is the outcome variable for the linear regression or logistic  regression model and the time of event or censoring in case of the Cox model, and finally the forth term is the event indicator variable for the Cox model taking the value 1 in case of an event or 0 in case of censoring at time y_.  The forth term would be NULL for either linear or logistic regression.  Currently the options for family are "guassian" for linear regression, "binomial" for logistic regression (both using the _stats_ glm() function) and "cox" for the Cox proportional hazards regression model using the coxph() function of the R _survival_ package.  If one sets track=1 the program will update progress in the R console, else for track=0 it will not.   |
    Before numerically summarizing the model fit, or inspecting the coefficient estimates, we plot the average deviances using the plot() function.  
```{r  Plot cross validation average deviances for a relaxed lasso model }
# Plot cross validation average deviances for a relaxed lasso model  
plot(cv.cox.fit)
```
```{r values1, include=FALSE}
lambda.min = round(log(cv.cox.fit$relaxed$lambda.min),digits=2)
lambda.min.nzero = cv.cox.fit$nzero[cv.cox.fit$relaxed$index[1,1]]
gamma.min = cv.cox.fit$relaxed$gamma.min
lambda.min.g0 = round(log(cv.cox.fit$relaxed$lambda.min.g0),digits=2)
lambda.min.g0.nzero=cv.cox.fit$nzero[cv.cox.fit$relaxed$index.g0[1]]
lambda.min.g1.nzero=cv.cox.fit$nzero[cv.cox.fit$index[1]]
```
In that to maximize the log-likelihoods is to minimize deviance we inspect these curves for a minimum.  The minimizing lambda is indicated by the left most vertical line, here about log(lambda) = `r lambda.min`.  The minimizing gamma is `r gamma.min` and described in the title. Whereas there is no legend here for gamma, when non-zero coefficients start to enter the model as when the penaly is large, here shown to the right, deviances will tend to be smaller for gamma = 0, greater for gamma = 1 and in between for other gammas values. From this figure we also see that at lambda=`r gamma.min` the deviances are very similar for the best models amongst those with gamma ranging from 0.5 to 1.  More relevant we see that the fully relaxed lasso (gamma=0) and indicated by the right most vertical line, achieves a "nearly" minimal deviance at about `r lambda.min.g0`. 
```{r  Plot coefficients from a cross validation informed lasso model}
# Plot coefficients informed by a cross validation   
plot(cv.cox.fit, coefs=TRUE)
```
In this plot of coefficients we use the same orientation for lambda as in the plot for deviances with larger values of the lambda penalty to the right and corresponding to fewer non-zero coefficients.  The displayed coefficients are for the minimizing gamma=`r gamma.min` as noted in the tile, and the minimizing lambda indicated by the vertical line.  Now, since the fully relaxed lasso model had a deviance almost that of the relaxed lasso model we also plot the coefficients using the option gam=0.  
```{r  Plot coefficients from a cross validation informed lasso model - fully relaxed}
# Plot fully relaxed coefficients informed by a cross validation   
plot(cv.cox.fit, coefs=TRUE, gam=0)
```
This plot shows how the coefficients change for the un-penalized (fully relaxed) model with gamma=0 as lambda decreases.  In particular we see how new terms enter into the model as the lamba penalty decreases, the coefficients in general become slightly larger as lambda decreases while some terms decrease as lambda decreases.  This is not unexpected as omitted terms from the Cox model tend to bias coefficients toward 0 more than increase the standard error.  We also see the number of model non-zero coefficients, `r lambda.min.g0.nzero`, to be substantially less than the `r lambda.min.nzero` from the relaxed lasso fit and the `r lambda.min.g1.nzero` from the fully penalized lasso fit.  
|    The summary() function describes the relaxed lasso fit informed by CV. 
```{r  Summarize relaxed lasso model fit informed by cross validation }
# Summarize relaxed lasso model fit informed by cross validation 
summary(cv.cox.fit)
```
In the summary output we first see the relaxed lasso model fit based upon the (lambda, gamma) pair which minimizes the cross validated average deviance.  Next is the model fit based upon the lambda that minimizes the cross validated average deviance along the path where gamma=0, that is among the fully relaxed lasso models. After that is information on the fully penalized lasso fit, but without the actual coefficient estimates.  These estimates can be printed using the option _printg1=TRUE_, but are suppressed by default for space.  Finally, the order that coefficients enter the lasso model as the penalty is decreased is provided, which gives some indication of relative model importance of the coefficients.  Because, though, the differences in successive lambda values used in the numerical algorithms may allow multiple new terms to enter into the model between successive numerical steps, the ordering in this list may not be strict.  If the user would want they could read lambda from output$lambda, set up a new lambda with finer steps and rerun the model.  Our experience though is that this does not generally lead to a meaningfully different model and so is not done by default or as option.  |
    One can as well use the predict() function to get the coefficients for the lasso model, or the xs_new*beta for a new design matrix xs_new.  In contrast to the summary() function which simply displays coefficients, the predict() function provides an outpout object in vector form (or a list with two vectors) and so can more easily be used for further calculations.  By default the summary() function will use the (lambda, gamma) pair that minimizes the average CV deviances.  One can also specify lam=NULL and gam=1 to use the fully penalized lasso best fit, that use the solution that minimizes the CV deviance with respect to lambda while holding gamma=1, or gam=0 to use the fully relaxed lasso best fit, that is minimizes while holding gamma=0.  One can also numerically specify both lam for lambda and gam for gamma.  Within the package lambda and gamma usually denote vectors for the search algorithm and so other names are used uere.      
```{r  Get coefficients and predicteds}
# Get coefficients
beta = predict(cv.cox.fit)
# Print out the non-zero coefficients
beta$beta
```
```{r  Get predicteds}
# Print out all coefficients
beta$beta_
```
``` {r predicteds }
# Get the predicteds (linear predictors) for the original data set
predicteds = predict(cv.cox.fit, xs)
# Print out the first few predicteds
predicteds[1:20]
```
# Model fit without cross validation 
|     We can as well fit a relaxed lasso model without doing a CV.  For this case one can still plot the coefficients but when the minimizing lambda and gamma are not informed by CV one is to specify which gamma should be used for the plots.  By default gamma=1, i.e. for the fully penalized lasso model, is used for the plots.  One can plot the coefficient estimates for different gamma values, but these will usually be more meaningful when informed by the CV "tuned" hyperparameters values for lambda and gamma.  One can also use the predict() function, again to output either coefficients or predicteds, i.e. xs_new*beta for a new design matrix xs_new.  Such predicteds are often, for example in coxph(), included in the analysis output object under the name linear.predictors.       
```{r  Simple lasso model fit (no cross validation done) 1}
# Fit a model without cross validation  
cox.fit = suppressWarnings( glmnetr(xs, NULL, y_, event, family="cox") )
```
```{r  Simple lasso model fit (no cross validation done) 2}
# Plot coefficients of the fully relaxed lasso model 
plot(cox.fit, gam=0)
```
```{r  Simple lasso model fit (no cross validation done) 3}
# Plot coefficients of the fully penalized lasso model
plot(cox.fit, gam=1)
```
```{r  Simple lasso model fit (no cross validation done) 4}
# Get an arbitrary set of coefficients for this example  
lam = cox.fit$lambda[min(20,length(cox.fit$lambda))]
predict(cox.fit,lam=lam,gam=1)$beta
```
# Nested cross validation 
|     Because the values for lambda and gamma informed by CV are specifically chosen to give a best fit, model fit statistics for the CV derived model will be biased.  To address this one can perform a CV on the CV derived estimates, that is a nested cross validation as argued for in SRDM ( Simon R, Radmacher MD, Dobbin K, McShane LM. Pitfalls in the Use of DNA Microarray Data for Diagnostic and Prognostic Classification. J Natl Cancer Inst (2003) 95  (1):  14-18. https://academic.oup.com/jnci/article/95/1/14/2520188 ).  This is done here by the nested.glmnetr() function.   

```{r  Nested cross-validation model evaluation}
# A nested cross validation to evaluate a cross validation informed lasso model fit 
# nested.cox.fit = nested.glmnetr(xs,NULL,y_,event,family="cox",track=1) 
nested.cox.fit = suppressWarnings(nested.glmnetr(xs,NULL,y_,event,family="cox",
      doann=1, dorpart=0, ensemble=c(1,0,0,0, 0,1,0,1), folds_n=10, track=0)) 
summary(nested.cox.fit)
#names(nested.cox.fit)
```
|     Before providing analysis results the output first reports sample size and since this is for a Cox regression, the number of events, followed by the number of predictors and the df (degrees of freedom) of the design matrix, as well as some information on "Tuning parameters".  For the lasso model the tuning parameter that are not informed by the cross validation (CV) is the number of folds used in the CV and nested CV.  When using CV to inform a stepwise procedure regression as described in JWHT (James, Witten, Hastie and Tibshirani, An Introduction to Statistical Learning with applications in R, 2nd ed., Springer, New York, 2021) the user should specify the maximum number of steps unless the number of predictors is small.  We have found in practice that in general the lasso does better than the stepwise procedure so we do not present results here.  (The tuned stepwise fits also take a long to run, possibly a part of the earlier motivation for the relaxed lasso model development.)  One can also compare the performance of the lasso with that of a gradient boosting machine (GBM here implemented using the extreme gradient boosting and the _xgboost_ package) by specifying doxgb=1, which we have not done in this call.  We do though compare the fit with that of an Artificial Neural Network (ANN) model by specifying doann=1.          
|     Next are the nested cross validation results.  First are the per record (or per event in case of the Cox model) log-likelihoods which reflect the amount of information in each observation.  Since we are not using large sample theory to base inferences we feel the per record are more intuitive.  Next are the average number of model terms which reflect the complexity of the different models, even if in a naive sense, then the linear calibration coefficients obtained by regressing the predicteds on outcome, followed by the agreement statistics, here concordance.  These nested cross validated concordances should be essentially unbiased for the given design, unlike the naive concordances where the same data are used to derive the model and calculate the concordances (see SRDM). 
|     In addition to evaluating the CV informed relaxed lasso model using another layer of CV, the nested.glmnetr() function also runs cv.glmnetr() based upon the whole data set.  Here we see, not unexpectedly, that the concordances estimated from the nested CV are slightly smaller than the concordances naively calculated using the original dataset.  Depending on the data the nested CV and naive agreement measures, here concordance, can be very similar or disparate.  
|     Following JWHT we provide information on the minimizing lasso models as well as the "1SE" models, which may be near to the minimizing lasso model fits, but of simpler nature.  We though focus on the minimizing lasso fits recognizing that relaxed lasso and fully relaxed lasso fits generally provide models of simpler form while still optimizing a fit.    
|     A summary for the CV fit can be produced by using the summary() function directly on a nested.glmnetr() output using the option _cvfit=TRUE_.  Else one can also extract the CV fit by extracting the object$cv.glmnet.fit, where object is the output object obtained when running nested.glmnetr().  The plot() and predict() functions can be applied direclty to a nested.glmnetr() object without the _cvfit_ option for futher evaluation or calculations for the CV model fit.
```{r  Summarize the cv.glmnetr model contained in a nested.glmnetr output object}
# Summary of the CV fit from a nested CV output 
summary(nested.cox.fit, cvfit=TRUE)
```
Observe, the summary here is slightly different than obtained above running cv.glmnetr().  This is because the model is derived using a new call (instance) of the cv.glmnetr() function, and each CV uses by default a new random partitioning of the data. 
```{r  Plot the cv.glmnetr model deviances contained in a nested.glmnetr output object}
# Plot CV deviances from a nested CV output 
plot(nested.cox.fit)
```
and 
```{r  plot the coefficients form a cv.glmnetr model contained in a nested.glmnetr output object}
# Plot coefficients from a nested CV output
plot(nested.cox.fit, coefs=TRUE)
```
Summarizing, the summary() function with the _cvfit_=TRUE option as well as the plot() and predict() functions for a nested.glmnetr() object are then essentially the same as those for a cv.glmnetr() output object. The summary() function without the _cvfit_=TRUE option, though, regards the evaluation of the cv.glmnetr() fit and is different. 
```{r  Extract the embeded cv.glmnetr output }
# ... or use the predict() function on the CV fit embedded in the nested CV output
predict(nested.cox.fit)$beta
```

Again, the plots and summary outputs from the nested.glmnetr() output are sightly different from what we saw above when summarizing the cv.glmnetr() output due to random data partitions for the CV folds. 

# More example data and relaxed lasso fits
|     The glmnetr.simdata() can be used to obtain example data not only survival analyses but also for linear models and logistic models.  The glmnetr.simdata() output object list contains not only xs for the predictor matrix, yt for time to event or censoring and event for event indication but also y_ for a normally distributed random variable for the linear model setting and yb for the logistic model setting.  Below we show examples extracting and analyzing simulated data and for the linear model and logistic model structures.
```{r Linear and logistic regressions}
#======================================================
# use the same simulated data output object from above, that is from the call
# simdata=glmnetr.simdata(nrows=1000, ncols=100, beta=NULL)
#
# extract linear regression model data
# xs = simdata$xs      # just as a comment as we did this above
yg = simdata$y_        # vector of Gaussian (normal) outcomes
# run a linear regression lasso model
cv.lin.fit = suppressWarnings(cv.glmnetr(xs,NULL,yg,NULL,family="gaussian",track=0))
summary(cv.lin.fit)
# plot(cv.lin.fit, coefs=TRUE)
#
# extract logistic regression model data
# xs = simdata$xs      # just as a comment as we did this above
yb = simdata$yb        # vector of binomial (0 or 1) outcomes
# run a logistic regression lasso model
cv.bin.fit = suppressWarnings(cv.glmnetr(xs,NULL,yb,NULL,family="binomial",track=0)) 
summary(cv.bin.fit)
# plot(cv.bin.fit, coefs=TRUE) 
```
# Further model assessment
|     One can also fit a spline to the predicteds obtained form the predict() functions.  This may help to understand nonlinearities in the predicteds, but may also give inflated hazard ratios.    
```{r Calibration plot}
# Get predicteds from CV relaxed lasso model embedded in nested CV outputs & Plot
xb.hat = predict( object=cv.cox.fit , xs_new=xs, lam=NULL, gam=NULL, comment=FALSE) 
# describe the distribution of xb.hat 
round(1000*quantile(xb.hat,c(0.01,0.05,0.1,0.25,0.5,0.75,0.90,0.95,0.99)))/1000 
# Fit a spline to xb.hat uisng coxph, and plot 
fit1 = coxph(Surv(y_, event) ~ pspline(xb.hat))
termplot(fit1,term=1,se=TRUE)
```
From this spline fit we see the predicteds are approximately linear with the log hazard ratio. 