---
title: "An Overview of glmnetr"
author: "Walter K. Kremers, Mayo Clinic, Rochester MN"
date: "30 December 2023"
output: pdf_document 
vignette: >
  %\VignetteIndexEntry{An Overview of glmnetr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(survival)
library(Matrix)
library(xgboost)
library(smoof)
library(mlrMBO)
library(ParamHelpers)
library(randomForestSRC)
library(rpart)
library(torch)
source("~/Documents/LASSO/glmnetr/R/_load_glmnetr_dev_231230.R" , echo=TRUE)
```
# The Package 
|   The nested.glmnetr() function of the 'glmnetr' package allows the user to fit multiple machine learning models on a common dataset with a single function call allowing an efficient comparison of different modeling approaches.  Additionally this function uses cross validation to estimate model performances for these different modeling approaches.  As most of these machine learning models choose hyperparameters informed by a cross validation or some sort of out of bag (OOB) performance measure, the nested.glmnetr() function provides model performance estimates based upon either a nested cross validation or analogous approach.  Measures of model performance include concordances for survival time and binomial outcomes and R-squares for quantitative numeric outcomes, as well as deviances and linear calibration coefficients.  Too often one sees performance reports including things like sensitivity, specificity or F1 scores in absence of any consideration of calibration.  Whereas linear calibration does not exhaust the needs of calibration considerations, it does provide a first high level insight.  As the purpose of the function is to not only describe performance but to derive the models, each of the fitted models as well as performance measures are stored in a single output object.

|   The nested.glmnetr() function fits cross validation informed Relaxed lasso, Artificial Neural Network (ANN), gradient boosting machine ('xgboost'), Random Forest ('RandomForestSRC'), Recursive Partitioning ('RPART') and step wise regression models.  As run times may be long, the user specifies which of these models to fit.  By default only the lasso model suite is fit, including the (standard) lasso, relaxed lasso, fully relaxed lasso (gamma=0) and the ridge regression model.  (The program was originally written to simply compare the lasso and stepwise regression models and thus this inclusion of the lasso by default, as well as the program name.)  By default model performances are calculated using cross validation but if the goal is to only fit the models this can be done using the option do_ncv=0.

|   As with the 'glmnet' package, tabular and graphical summaries can be generated using the summary and plot functions.  Use of the 'glmnetr' has many similarities to the 'glmnet' package and the user may benefit by a review of the documentation for the 'glmnet' package <https://cran.r-project.org/package=glmnet>, with the "An Introduction to 'glmnet'" and "The Relaxed Lasso" being especially helpful in this regard.

|   For some datasets, for example when the design matrix is not of full rank, 'glmnet' may have very long run times when fitting the relaxed lasso model, from our experience when fitting Cox models on data with many predictors and many patients, making it difficult to get solutions from either glmnet() or cv.glmnet().  This may be remedied with the 'path=TRUE' option when calling cv.glmnet().  This option is not described in the 'glmnet' Reference Manual but is described in the 'glmnet' "The Relaxed Lasso" vignette.  In the 'glmnetr' package we provide a similar workaround when fitting the non penalized relaxed model where gamma=0.

|   When fitting not a relaxed lasso model but an elastic-net model, then the R-packages 'nestedcv' <https://cran.r-project.org/package=nestedcv>, 'glmnetSE' <https://cran.r-project.org/package=glmnetSE> or others may provide greater functionality when performing a nested CV. 
|     
# Data requirements
|   The basic data elements for input to the _glmnetr_ analysis programs are similar to those of _glmnet_ and include 1) a matrix of predictors and 2) an outcome variable in vector form.  For the different machine learning modeling approaches the package is set up to model generalizations of the proportional hazards Cox survival model, the "binomial" outcome logistic model and linear regression for well behave numerical outcomes treated as if guassian in distribution.  When fitting the Cox model the outcome model variable is interpreted as the "time" variable in the Cox model, and one must also specify 3) a variable for event, again in vector form, and optionally 4) a variable for start time, also in vector form.  Row i of the predictor matrix and element i of the outcome vector(s) are to include the data for the same sampling unit.

|   The input vectors may optionally be specified as column matrices (with only one column each) in which case the column name will be kept and expressed in the model summaries.

# An example dataset
|   To demonstrate usage of _glmnetr_ we first generate a data set for analysis, run an analysis and evaluate using the plot(), summary() and predict() functions.

|   The code 
```{r Simuate survival data}
# Simulate data for use in an example relaxed lasso fit of survival data
# first, optionally, assign a seed for random number generation to get replicable results  
set.seed(116291949) 
simdata=glmnetr.simdata(nrows=1000, ncols=100, beta=NULL)
```
generates simulated data for analysis.  We extract data in the format required for input to the _glmnetr_ programs.
```{r Extract data from the simulation output object}
# Extract simulated survival data 
xs = simdata$xs        # matrix of predictors 
y_ = simdata$yt        # vector of survival times 
event = simdata$event  # indicator of event vs. censoring
```
Inspecting the predictor matrix we see 
```{r Inspect xs, the design matrix}
# Check the sample size and number of predictors
print(dim(xs)) 
# Check the rank of the design matrix, i.e. the degrees of freedom in the predictors 
rankMatrix(xs)[[1]]
# Inspect the first few rows and some select columns 
print(xs[1:10,c(1:12,18:20)])
```
# Performance of cross validation (CV) informed relaxed lasso model fit
|   Because the values for lambda and gamma informed by CV are specifically chosen to give a best fit, model fit statistics for the CV derived model will be biased.  To address this one can perform a CV on the CV derived estimates, that is a nested cross validation as argued for in SRDM ( Simon R, Radmacher MD, Dobbin K, McShane LM. Pitfalls in the Use of DNA Microarray Data for Diagnostic and Prognostic Classification. J Natl Cancer Inst (2003) 95  (1):  14-18. https://academic.oup.com/jnci/article/95/1/14/2520188 ).  We demonstrate the model performance evaluation by nested cross validation first for the lasso models with the evaluation of other machine learning models being similar.  For this performance evaluation we use the nested.glmnetr() function which first fits lasso models based upon all data and then performs the cross validation for calculation of concordances or R-squares, deviances and linear calibration summaries.

```{r  Fit a relaxed lasso model informed by cross validation }
set.seed(465783345) 
nested.cox.fit = suppressWarnings( nested.glmnetr(xs, NULL, y_, event, family="cox", 
                                   dostep=1, folds_n=10, track=0) ) 
```
Note, in the derivation of the relaxed lasso model fits, individual coefficients may be unstable even when the model may be stable which elicits warning messages.  Thus we "wrap" the call to nested.glmnetr() within the suppressWarnings() function to avoid excessive warning messages in this vignette.  The first term in the call to nested.glmnetr(), xs, is the design matrix for predictors.  The second input term, here NULL, is for the start time in case the (start, stop) time data setup is used in a Cox survival model fit. The third term is the outcome variable for the linear regression or logistic  regression model and the time of event or censoring in case of the Cox model, and finally the forth term is the event indicator variable for the Cox model taking the value 1 in case of an event or 0 in case of censoring at time y_.  The forth term would be NULL for either linear or logistic regression.  If one sets track=1 the program will update progress in the R console, else for track=0 it will not.

|   Before numerically summarizing the model fit, or inspecting the coefficient estimates, we inspect the average cross validation deviance using the plot function.
```{r  Plot cross validation average deviances for a relaxed lasso model }
# Plot cross validation average deviances for a relaxed lasso model  
plot(nested.cox.fit)
```
```{r values1, include=FALSE}
lambda.min = round(log(nested.cox.fit$cv_glmnet_fit$relaxed$lambda.min),digits=2)
lambda.min.nzero = nested.cox.fit$cv_glmnet_fit$nzero[nested.cox.fit$relaxed$index[1,1]]
gamma.min = nested.cox.fit$cv_glmnet_fit$relaxed$gamma.min
lambda.min.g0 = round(log(nested.cox.fit$cv_glmnet_fit$relaxed$lambda.min.g0),digits=2)
lambda.min.g0.nzero=nested.cox.fit$cv_glmnet_fit$nzero[nested.cox.fit$cv_glmnet_fit$relaxed$index.g0[1]]
lambda.min.R.nzero =nested.cox.fit$cv_glmnet_fit$nzero[nested.cox.fit$cv_glmnet_fit$relaxed$index[1,1]]
lambda.min.g1.nzero=nested.cox.fit$cv_glmnet_fit$nzero[nested.cox.fit$cv_glmnet_fit$index[1]]
```
In that to maximize the log-likelihood is to minimize deviance we inspect these curves for a minimum.  The minimizing lambda is indicated by the left most vertical line, here about log(lambda) = `r lambda.min`.  The minimizing gamma is `r gamma.min` and described in the title. Whereas there is no legend here for gamma, when non-zero coefficients start to enter the model as the penalty is reduced, here shown at the right, deviances tend to be smaller for gamma = 0, greater for gamma = 1 and in between for other gammas values. From this figure we also see that at lambda=`r gamma.min` the deviance is hardly distinguishable for gamma ranging from 0.5 to 1.  More relevant we see that the fully unpenalized lasso model fits  (gamma=0) shown in a black line with a black circle at the largest lambda, achieves a minimal deviance at about `r lambda.min.g0`, and highlighted by the right most vertical line.  The minimizing deviance for the fully relaxed lasso model is "nearly" that of the relaxed lasso model tuning for both lambda and gamma.   

|   A plot depicting model fits as a function of lambda is given in the next figure.  
```{r  Plot coefficients from a cross validation informed lasso model}
# Plot coefficients informed by a cross validation   
plot(nested.cox.fit, coefs=TRUE)
```
In this plot of coefficients we use the same orientation for lambda as in the plot for deviances with larger values of the lambda penalty to the right and corresponding to fewer non-zero coefficients.  The displayed coefficients are for the minimizing gamma=`r gamma.min` as noted in the tile, and the minimizing lambda indicated by the vertical line.  Since the fully relaxed lasso model had a deviance almost that of the relaxed lasso model we also plot the coefficients using the option gam=0.  
```{r  Plot coefficients from a cross validation informed lasso model - fully relaxed}
# Plot fully relaxed coefficients informed by a cross validation   
plot(nested.cox.fit, coefs=TRUE, gam=0)
```
In addition to simply showing how the coefficients change as the lambda penalty is decreased, this plot shows how the coefficients change for the un-penalized (fully relaxed) model with gamma=0 as lambda decreases.  In particular we see the coefficients become slightly larger in magnitude as the lambda penalty decreases and also as additional terms come into the model.  This is not unexpected as omitted terms from the Cox model tend to bias coefficients toward 0 more than increase the standard error.  We also see, as too indicated in the deviance plot, the number of model non-zero coefficients, `r lambda.min.g0.nzero`, to be substantially less than the `r lambda.min.R.nzero` from the relaxed lasso fit and the `r lambda.min.g1.nzero` from the fully penalized lasso fit.

|   As usual with R functions and packages we use the summary function to describe output.  Here the summary function displays a brief summary of the input data and program options before proceeding to describe model performances.  The data summary includes sample size, number of events, number of candidate model predictors, degress of freedom in these predictors as well as average deviance.  Model performances are displayed for the different lasso models, e.g. standard, relaxed, fully relaxed as well as the ridge regression and stepwise regression models.  
Hyperparameters considered for the lasso models include both the minimizing lambdas as well as the "1se" or "one standard deviation" lambdas for the relaxed lasso fit informed by CV.  Hyperparamters considered for stepwise regression were degress of freedom (df) and p, the p-value for entry into the regression equation, as discussed by JWHT (James, Witten, Hastie and Tibshirani, An Introduction to Statistical Learning with applications in R, 2nd ed., Springer, New York, 2021).  Performance measures include deviance, linear calibration coefficients and measures of agreement, here for the Cox model framework concordance.  Additionally there are the deviance and agreement from the whole sample.  Observe how the deviances are much larger for the whole sample than for the deviances for the leave out samples in the (outer) cross validation.  This is because the risk sets are larger for the whole sample leading to larger numbers derived at each event time.
```{r  Summarize relaxed lasso model perfromance informed by cross validation }
# Summarize relaxed lasso model performance informed by cross validation 
summary(nested.cox.fit)
```
From this output we also see the number of non-zero coefficients in the different models, reflecting model complexity, along with the linear calibration coefficients obtained by regressing the outcome on the predicteds.

|   In addition to evaluating the CV informed relaxed lasso model using another layer of CV, the nested.glmnetr() function also calculates model performances based upon all data for the model based upon all data.  Here we see, not unexpectedly, that the concordances estimated from the nested CV are slightly smaller than the concordances naively calculated using the original dataset.  Depending on the data the nested CV and naive agreement measures, here concordance, can be very similar or disparate.
   
|   A summary of the actual lasso model fit can be gotten by using the cvfit=1 option in the summary() call.  
```{r  Summarize relaxed lasso model fit informed by cross validation }
# Summarize relaxed lasso model fit informed by cross validation 
summary(nested.cox.fit, cvfit=1)
```
In the summary output we first see the relaxed lasso model fit based upon the (lambda, gamma) pair which minimizes the cross validated average deviance.  Next is the model fit based upon the lambda that minimizes the cross validated average deviance along the path where gamma=0, that is among the fully relaxed lasso models. After that is information on the fully penalized lasso fit, but without the actual coefficient estimates.  These estimates can be printed using the option _printg1=TRUE_, but are suppressed by default for space.  Finally, the order that coefficients enter the lasso model as the penalty is decreased is provided, which gives some indication of relative model importance of the coefficients.  Because, though, the differences in successive lambda values used in the numerical algorithms may allow multiple new terms to enter into the model between successive numerical steps, the ordering in this list may not be strict.  If the user would want they could read lambda from output$lambda, set up a new lambda with finer steps and rerun the model.  Our experience though is that this does not generally lead to a meaningfully different model and so is not done by default or as option.

|   One can as well use the predict() function to get the coefficients for the lasso model, which is done by not specifying a predictor matrix.  If one specifies a new design matrix xs_new, then the predicteds xs_new*beta are generated.  In contrast to the summary function which simply displays coefficients, the predict function provides an output object in vector form (actually a list with two vectors) and so can more easily be used for further calculations.  By default the summary function will use the (lambda, gamma) pair that minimizes the average CV deviances.  One can also specify lam=NULL and gam=1 to use the fully penalized lasso best fit, that use the solution that minimizes the CV deviance with respect to lambda while holding gamma=1, or gam=0 to use the fully relaxed lasso best fit, that is minimizes while holding gamma=0.  One can also numerically specify both lam for lambda and gam for gamma.  Within the package lambda and gamma usually denote vectors for the search algorithm and so other names are used here.    
```{r  Get coefficients and predicteds}
# Get coefficients
beta = predict(nested.cox.fit)
# Print out the non-zero coefficients
beta$beta
# Print out all coefficients
beta$beta_
# Get the predicteds (linear predictors) for the original data set
predicteds = predict(nested.cox.fit, xs)
# Print out the first few predicteds
predicteds[1:20]
```
# Nested cross validation for multiple machine learning models
|   Here we evaluate multiple machine learning models, in particular the lasso, XGB, random forest and neural network models.  For this example too we perform an analysis for the generalizations of linear regression in contrast to the Cox model in the last example.  The glmnetr.simdata() function used above actually creates an output object list contains not only xs for the predictor matrix, yt for time to event or censoring and event for event indication but also y_ for a normally distributed random variable for the linear model setting and yb for the logistic model setting.  
```{r  Nested cross-validation model for multiple machine learning models}
# Nested cross validation evaluated machine learning model suite and guassian errors
# use the same simulated data output object from above, that is from the call
# simdata=glmnetr.simdata(nrows=1000, ncols=100, beta=NULL)
#
# extract linear regression model data
yg = simdata$y_                           # vector of Gaussian (normal) outcomes
# Get the ML model fits 
nested.gau.fit = suppressWarnings(nested.glmnetr(xs,NULL,yg,NULL,family="gaussian",
      dolasso=1, doxgb=1, dorf=1, doann=1, ensemble=c(1,0,0,0, 0,1,0,1), folds_n=10, 
      seed=219301029, track=0)) 
summary(nested.gau.fit)
```

|   Here we see a set of machine learning models evaluated together.  Information displayed is similar to what we saw before with the main difference being that for many ML models there are no "number of non-zero terms" like for the lasso.  All evaluations are based upon the same folds for the outer loop of the cross validation.  Those models informed by cross validation in identification of hyperparameters, i.e. lasso, neural network and stepwise, use the same folds in the inner cross validation making the comparisons of model performance between models more stable.  For the models based upon other random splitings, i.e. XGB, random forest and RPART, the same seed is set using set.seed() before each model call facilitating replicability of results.

|   The indiviudal model fits are all captured in the nested.glmnetr() output object with names like cv_glmnet_fit, xgb.simple.fit, xgb.tuned.fit, rf.fit, cv.stepreg.fit and ann_fit_X with X corresponding to the ensemble designation.  cv_glmnet_fit has a similar yet different format to that of cv.glmnet() by including further fit information.  The XGB outputs are direct outputs from 'xgboost' xgb.train(). The rf.fit object contains the output from rfsrc() in the object rf.fit$rf_tuned along with other information used for tuning.  The ann_fit_X objects are derived using the R 'torch' package and take on their own format for logistical reasons.  See the 'Using ann_tab_cv' vignette.  Cross validation information form the individual outer folds are are contained in datasets like xx.devian.cv, xx.lincal.cv, xx.agree.cv for further processing by the summary() function or by the user.  For example  
```{r  Get R_square perforamnce summary from nested.glmnetr call }
# Manually calculate CV R_square for lasso models 
corr.cv = nested.gau.fit$lasso.agree.cv
avecorr = colMeans(corr.cv)
R_square = avecorr ^2 
R_square
```
These numbers are consistent witht the output from the summary() call.

|   In this and the previous examples using nested.glmnetr() we specified values for seed.  This assures that the user can test the program with their own installation and get the same results.  Typically in practice one can leave the seed unspecified and the program will generate its own seeds and store these in the output object (object$seed) for future reference.  One should be cautious of using set.seed() in one's own code as this too will effect the pseudo randomness used in the ML calculations and could unwantingly yield identical results when pseudo independent runs are intended.  

# Further model assessment
|   Further model assessment can performed based upon the predicteds from the predict functions.  For example, ene can model the outcomes based upon a spline for the X*Beta hats from the predicteds.  This may help to understand potential nonlinearities in the model, but may also give inflated hazard ratios.    
```{r Calibration plot}
# Get predicteds from CV relaxed lasso model embedded in nested CV outputs & Plot
xb.hat = predict( object=nested.cox.fit , xs_new=xs, lam=NULL, gam=NULL, comment=FALSE) 
# describe the distribution of xb.hat 
round(1000*quantile(xb.hat,c(0.01,0.05,0.1,0.25,0.5,0.75,0.90,0.95,0.99)))/1000 
# Fit a spline to xb.hat uisng coxph, and plot 
fit1 = coxph(Surv(y_, event) ~ pspline(xb.hat))
summary(fit1)
termplot(fit1,term=1,se=TRUE)
```
From this spline fit we see the predicteds are approximately linear with the log hazard ratio. 